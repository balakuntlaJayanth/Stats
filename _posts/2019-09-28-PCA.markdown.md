---
layout: post
title:  "Principal Component Analysis"
date:   2019-09-29 22:02:38 +0530
categories: jekyll update
---

## Principal Component Analysis (PCA)

Principal component Analysis is a multivariate statistical method to identify patterns and interprete patterns in the data when multiple variables are present in the data to interprete.

PCA try's to identify the linear or continous variables in the data. PCA eliminates redundant variables in the dataset and preserving important 
information. Final variables obtained after removing reduandant variables
are called Principle Components

PCA in short is a method to show strong patterns from large and composite data sets. 

## PCA implementation in Python


{% highlight python %}

import numpy as np
import matplotlib.pyplot as plt 
import pandas as pd  
import seaborn as sns 

{% endhighlight %}

{% highlight python %}

df1 = pd.read_csv('table4.txt', sep='\t')
print(df1.head())

{% endhighlight %}


<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CR4</th>
      <th>CR5</th>
      <th>TR4</th>
      <th>TR5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.912188</td>
      <td>4.013982</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>1.908565</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.890235</td>
      <td>6.020973</td>
      <td>2.101308</td>
      <td>15.268521</td>
    </tr>
    <tr>
      <th>3</th>
      <td>24.451176</td>
      <td>28.097873</td>
      <td>26.266345</td>
      <td>18.131368</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1.956094</td>
      <td>4.013982</td>
      <td>0.000000</td>
      <td>3.817130</td>
    </tr>
  </tbody>
</table>
</div>


{% highlight python %}

df1.shape

{% endhighlight %}


    (57773, 4)



## Data Normalization 

Data Normalization is an important process in correcting the bias in the data. PCA is largely affected by scales and different features might have different scales. So it is better to Normalise data before finding PCA components. Sklearn's StandardScaler Normalises data to Zscores. It is important step in many of the machine learning algorithms. 

{% highlight python %}

from sklearn.decomposition import PCA
import pandas as pd
import re
import numpy as np
from itertools import groupby
import matplotlib.pyplot as plt

from sklearn.preprocessing import StandardScaler
x = StandardScaler().fit_transform(df1)
S_columns = list(df1.columns.values)

{% endhighlight %}


{% highlight python %}

x = pd.DataFrame(x, columns=list(df1.columns.values))

{% endhighlight %}

After Normalization


{% highlight python %}

x.head()

{% endhighlight %}




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>CR4</th>
      <th>CR5</th>
      <th>TR4</th>
      <th>TR5</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-0.152830</td>
      <td>-0.151813</td>
      <td>-0.153595</td>
      <td>-0.156670</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.153521</td>
      <td>-0.152533</td>
      <td>-0.153595</td>
      <td>-0.156312</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-0.152657</td>
      <td>-0.151453</td>
      <td>-0.153227</td>
      <td>-0.153805</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-0.149204</td>
      <td>-0.147493</td>
      <td>-0.148995</td>
      <td>-0.153268</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-0.153175</td>
      <td>-0.151813</td>
      <td>-0.153595</td>
      <td>-0.155954</td>
    </tr>
  </tbody>
</table>
</div>



## Calculating PCA involves following steps:

-     Calculating the covariance matrix
-     Calculating the eigenvalues and eigenvector
-     Forming Principal Components
-     Projection into the new feature space

In PCA(), n_components specifies how many components are returned after fit and tranformation.


{% highlight python %}

pca_out = PCA()
x_new = pca_out.fit_transform(x)

{% endhighlight %}

{% highlight python %}

    prop_var = pca_out.explained_variance_ratio_
    cum_prop_var = np.cumsum(prop_var)
    rotation = pca_out.components_
    num_pc = pca_out.n_features_
    pc_list = list(range(1, num_pc+1))
    pc_list = ["PC"+str(w) for w in pc_list]
    pca_df_var = [prop_var, cum_prop_var]
    pca_df_out = pd.DataFrame.from_dict(dict(zip(pc_list, zip(*pca_df_var))))
    pca_df_rot_out = pd.DataFrame.from_dict(dict(zip(pc_list, rotation)))
    pca_df_out.rename(index={0: "Proportion of Variance", 1: "Cumulative proportion"}, inplace=True)
    print("Component summary\n")
    print(pca_df_out)
    print("\nLoadings\n")
    pca_df_rot_out['sample'] = S_columns
    pca_df_rot_out = pca_df_rot_out.set_index('sample')
    del pca_df_rot_out.index.name
    print(pca_df_rot_out)

{% endhighlight %}

    Component summary
    
                                 PC1       PC2       PC3       PC4
    Proportion of Variance  0.976308  0.021478  0.001890  0.000323
    Cumulative proportion   0.976308  0.997786  0.999677  1.000000
    
    Loadings
    
              PC1       PC2       PC3       PC4
    CR4  0.501281 -0.432229 -0.551641  0.507531
    CR5  0.499029 -0.550365  0.373845 -0.555255
    TR4  0.497545  0.606344 -0.432283 -0.444890
    TR5  0.502132  0.377656  0.607506  0.485980


{% highlight python %}

def screeplot(obj="pcascree"):
    y = [x * 100 for x in obj[1]]
    plt.bar(obj[0], y)
    plt.xlabel('PCs', fontsize=12, fontname="sans-serif")
    plt.ylabel('Proportion of variance (%)', fontsize=12, fontname="sans-serif")
    plt.xticks(fontsize=7, rotation=70)
    plt.show()

{% endhighlight %}


{% highlight python %}

    pcascree = [pc_list, prop_var]
    # screeplot
    screeplot(obj=pcascree)

{% endhighlight %}

### Figure 1: Scree plot

Scree plot is generally histogram or line plot represent Principal co-ordinates in the X-axis and Variance in Y-axis. If maximum variance 
is captured in more than 3 Principal components , then PCA may not be best way to represent the calculate variance.

<img src="https://github.com/balakuntlaJayanth/Stats/blob/master/images/output_14_0.png">

{% highlight python %}

def pcaplot(x="x", y="y", z="z", labels="S_columns", var1="var1", var2="var2", var3="var3"):
    for i, varnames in enumerate(labels):
        plt.scatter(x[i], y[i])
        plt.text(x[i], y[i], varnames, fontsize=10)
    plt.xlabel("PC1 ({}%)".format(var1), fontsize=12, fontname="sans-serif")
    plt.ylabel("PC2 ({}%)".format(var2), fontsize=12, fontname="sans-serif")
    plt.tight_layout()
    plt.show()

{% endhighlight %}

{% highlight python %}

pcaplot(x=rotation[0], y=rotation[1], z=rotation[2], labels=S_columns, var1=round(prop_var[0]*100, 2), var2=round(prop_var[1]*100, 2),
            var3=round(prop_var[2] * 100, 2))

{% endhighlight %}

### Figure 2: PCA plot

<img src="https://github.com/balakuntlaJayanth/Stats/blob/master/images/output_16_0.png">


### Figure 2: PCA Scatter plot

{% highlight python %}

plt.scatter(x_new[:, 0], x_new[:, 1])

{% endhighlight %}




    <matplotlib.collections.PathCollection



<img src="https://github.com/balakuntlaJayanth/Stats/blob/master/images/output_17_1.png">


### Figure 3: PCA Heat map

PCA heatmap represent 

{% highlight python %}

import seaborn as sns
ax = sns.heatmap(pca_out.components_,
                 cmap='YlGnBu',
                 yticklabels=[ "PCA"+str(x) for x in range(1,pca_out.n_components_+1)],
                 xticklabels=list(x.columns),
                 cbar_kws={"orientation": "horizontal"})
ax.set_aspect("equal")

{% endhighlight %}



<img src="https://github.com/balakuntlaJayanth/Stats/blob/master/images/output_18_0.png">


### PCA Biplot

Biplot is an interesting plot and contains lot of useful information.

It contains two plots: 

1.   *PCA scatter plot* which shows first two component ( We already plotted this above)
2.   *PCA loading plot* which shows how strongly each characteristic influences a principal component.

**PCA Loading Plot:**  All vectors start at origin and their projected values on components explains how much weight they have on that component. Also , angles between individual vectors tells about correlation between them.

More about biplot [here](http://www.nonlinear.com/support/progenesis/comet/faq/v2.0/pca.aspx)

{% highlight python %}

def biplot(score,coeff,labels):
    xs = score[:,0]
    print(xs)
    ys = score[:,1]
    n = coeff.shape[0]
    scalex = 1.0/(xs.max() - xs.min())
    scaley = 1.0/(ys.max() - ys.min())
    plt.scatter(xs * scalex,ys * scaley, marker='.', s=8,c='purple')
    for i in range(n):
        plt.arrow(0, 0, coeff[i,0], coeff[i,1],color = 'r',alpha = 0.5)
        if labels is None:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, "Var"+str(i+1), color = 'g', ha = 'center', va = 'center')
        else:
            plt.text(coeff[i,0]* 1.15, coeff[i,1] * 1.15, labels[i], color = 'g', ha = 'center', va = 'center')
    plt.xlim(-1,1)
    plt.ylim(-1,1)
    plt.xlabel("PC{}".format(1))
    plt.ylabel("PC{}".format(2))
    plt.grid()
    plt.show()

{% endhighlight %}

{% highlight python %}

biplot(x_new[:,0:2],np.transpose(pca_out.components_[0:2, :]),S_columns)

{% endhighlight %}

    [-0.30745951 -0.30798519 -0.30557164 ...  0.88300568 -0.30798536
      0.59186992]

### Figure 4: PCA Biplot

<img src="https://github.com/balakuntlaJayanth/Stats/blob/master/images/output_20_1.png">

##  References:

* [http://setosa.io/ev/principal-component-analysis/](http://setosa.io/ev/principal-component-analysis)
* [https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c](https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c)
* [https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/](https://blog.bioturing.com/2018/06/18/how-to-read-pca-biplots-and-scree-plots/)
* [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis)
* [https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0](https://medium.com/@aptrishu/understanding-principle-component-analysis-e32be0253ef0)